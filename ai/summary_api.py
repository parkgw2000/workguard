import os
import torch
import logging
import traceback
from fastapi import FastAPI, Request
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, AutoConfig

# ------------------------------------------------------------
# âš™ï¸ í™˜ê²½ ì„¤ì • (CUDA, ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ, ê²½ê³  ìµœì†Œí™”)
# ------------------------------------------------------------
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
logging.disable(logging.INFO)
logging.disable(logging.WARNING)

app = FastAPI()

# ------------------------------------------------------------
# ğŸ“¦ ëª¨ë¸ ì •ë³´
# ------------------------------------------------------------
model_name = 'gogamza/kobart-base-v2'
model_path = 'C:/Users/1/Desktop/Downloads/workguard/src/main/resources/models/ai/checkpoint-26606/'

params = {
    'num_beams': 4,
    'max_length': 200,
    'length_penalty': 0.5,
    'no_repeat_ngram_size': 3,
    'early_stopping': True
}

# ------------------------------------------------------------
# ğŸ§  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
# ------------------------------------------------------------
def load_model_and_tokenizer():
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    config = AutoConfig.from_pretrained(model_path)
    model = BartForConditionalGeneration.from_pretrained(model_path, config=config)
    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    model.eval()
    print(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {device})")
    return model, tokenizer, device

# ì „ì—­ ë³€ìˆ˜ë¡œ ë¡œë“œ (ì²« ìš”ì²­ ì§€ì—° ìµœì†Œí™”)
model, tokenizer, device = load_model_and_tokenizer()

# ------------------------------------------------------------
# ğŸ§¹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# ------------------------------------------------------------
def preprocess_text(text: str) -> str:
    return text.replace('_x000D_', '').replace('\r', '').replace('\n', ' ').strip()

# ------------------------------------------------------------
# ğŸª„ ìš”ì•½ API
# ------------------------------------------------------------
@app.post("/summarize")
async def summarize(request: Request):
    try:
        data = await request.json()
        text = data.get('text', '')
        print(f"ì…ë ¥ í…ìŠ¤íŠ¸: {repr(text[:100])}...")  # ì• 100ìë§Œ ì¶œë ¥

        if not text or not text.strip():
            return {"error": "No valid text provided"}

        text = preprocess_text(text)

        input_ids = tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=1024)
        if input_ids.numel() == 0:
            return {"error": "Tokenized input is empty"}

        input_ids = input_ids.to(device)

        with torch.no_grad():  # ì†ë„ + ë©”ëª¨ë¦¬ ìµœì í™”
            summary_ids = model.generate(
                input_ids,
                num_beams=params['num_beams'],
                max_length=params['max_length'],
                eos_token_id=tokenizer.eos_token_id,
                length_penalty=params['length_penalty'],
                no_repeat_ngram_size=params['no_repeat_ngram_size'],
                early_stopping=params['early_stopping']
            )

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print(f"ìƒì„± ìš”ì•½: {summary}")

        return {"summary": summary}

    except Exception as e:
        traceback.print_exc()
        return {"error": str(e)}
